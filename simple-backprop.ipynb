{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243484b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "73648efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "iris_df = pd.read_csv(iris_url, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n",
    "iris_np = iris_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "155e0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {'Iris-setosa': 0,\n",
    "            'Iris-versicolor': 1,\n",
    "            'Iris-virginica': 2}\n",
    "\n",
    "map_func = np.vectorize(lambda x: map_dict[x])\n",
    "iris_np[:, -1] = map_func(iris_np[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "69121007",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.concatenate((np.random.choice(np.arange(0, 50), size=10, replace=False), \n",
    "                               np.random.choice(np.arange(50, 100), size=10, replace=False), \n",
    "                               np.random.choice(np.arange(100, 150), size=10, replace=False)))\n",
    "train_indices = np.array(list(set(np.arange(0, 150)) - set(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dde03d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = iris_np[train_indices,:], iris_np[test_indices,:]\n",
    "\n",
    "def process(dataset):\n",
    "    X = dataset[:, :-1].astype(float)\n",
    "    y = dataset[:, -1].astype(int)\n",
    "    y_one_hot = np.zeros((len(y), max(y)+1))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return X, y_one_hot\n",
    "\n",
    "X, y = process(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "01f295d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "    def backward(self, input_gradient):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d6621da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyLayer(Layer):\n",
    "    def __init__(self, input_size, hidden_units):\n",
    "        self.weights = np.random.normal(size=(input_size, hidden_units))\n",
    "        self.bias = np.random.normal(size=hidden_units)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(self.weights.transpose(), self.input) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input_gradient):\n",
    "        self.weights_gradient = np.outer(self.input, input_gradient)\n",
    "        self.bias_gradient = input_gradient\n",
    "        self.input_gradient = np.dot(self.weights, input_gradient)\n",
    "        return self.input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "dc1623e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs\n",
      " [ 3.81709679 -3.25386426  7.69996012  0.12172383 -5.1335087  -4.08371983]\n",
      "\n",
      "Weights Gradient\n",
      " [[ 1.29818117 -1.09968535  6.06715682 -7.80064841 -4.61036132 11.80617658]\n",
      " [ 0.89090865 -0.75468603  4.16373508 -5.35338616 -3.16397345  8.10227805]\n",
      " [ 0.35636346 -0.30187441  1.66549403 -2.14135446 -1.26558938  3.24091122]\n",
      " [ 0.05090907 -0.04312492  0.23792772 -0.30590778 -0.18079848  0.46298732]]\n",
      "\n",
      "Bias Gradient\n",
      " [ 0.25454533 -0.21562458  1.18963859 -1.5295389  -0.90399242  2.31493658]\n",
      "\n",
      "Input Gradient\n",
      " [ 2.18958932 -1.20645166 -2.48033123 -0.33620507]\n"
     ]
    }
   ],
   "source": [
    "x0 = X[0]\n",
    "mul_layer = MultiplyLayer(4, 6)\n",
    "\n",
    "mul_layer.forward(x0)\n",
    "print(\"Outputs\\n\", mul_layer.output)\n",
    "\n",
    "mul_layer.backward(np.random.normal(size=6))\n",
    "print(\"\\nWeights Gradient\\n\", mul_layer.weights_gradient)\n",
    "print(\"\\nBias Gradient\\n\", mul_layer.bias_gradient)\n",
    "print(\"\\nInput Gradient\\n\", mul_layer.input_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a58576cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        map_grad = np.vectorize(lambda x: 1 if x > 0 else 0)\n",
    "        self.local_gradient = map_grad(self.input).astype(float)\n",
    "        self.output = np.maximum(np.zeros(input.shape), input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input_gradient):\n",
    "        self.gradient = self.local_gradient * input_gradient # in the forward pass, zeros are handled, so the same zeros will still be zeros in the gradients\n",
    "        return self.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e1bb9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs\n",
      " [3.81709679 0.         7.69996012 0.12172383 0.         0.        ]\n",
      "Local Gradient\n",
      " [1. 0. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "z1 = mul_layer.output\n",
    "relu = ReLU()\n",
    "\n",
    "relu.forward(z1)\n",
    "print(\"Outputs\\n\", relu.output)\n",
    "print(\"Local Gradient\\n\", relu.local_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "306f37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        input_exp = np.exp(self.input)\n",
    "        sum_exp = sum(input_exp)\n",
    "        self.output = input_exp / sum_exp\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, input_gradient):\n",
    "        output_size = len(self.output)\n",
    "        self.local_gradient = np.ones((output_size, output_size))\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                if i == j:\n",
    "                    self.local_gradient[i][j] = self.output[i] - self.output[i]**2\n",
    "                else:\n",
    "                    self.local_gradient[i][j] = -1 * self.output[i] * self.output[j]\n",
    "        self.gradient = np.dot(self.local_gradient, input_gradient)\n",
    "        return self.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "66f55631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax\n",
      " [8.18783173e-01 1.74255283e-01 5.70374506e-07 1.09366913e-03\n",
      " 4.02686323e-05 5.82703533e-03]\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "\n",
    "softmax.forward(z1)\n",
    "print(\"Softmax\\n\", softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d9661f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input, label):\n",
    "        self.input = input\n",
    "        self.label = label\n",
    "        self.loss = -1 * np.dot(self.label, np.log(self.input))\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradient = -1 * np.divide(self.label, self.input)\n",
    "        return self.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "840172b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      " [5.1 3.5 1.4 0.2]\n",
      "\n",
      "y\n",
      " [1. 0. 0.]\n",
      "\n",
      "Layer 1 Output\n",
      " [-4.68775714  6.79915466 -6.13259108]\n",
      "\n",
      "Layer 1 Weights\n",
      " [[ 0.62075259  1.1640649  -0.84383394]\n",
      " [-1.9700508  -0.02372526 -0.78859462]\n",
      " [-0.79680868  0.45923137  0.1285176 ]\n",
      " [-1.85980172 -1.45545795 -0.16115065]]\n",
      "\n",
      "Prediction\n",
      " [1.02634196e-05 9.99987317e-01 2.41996271e-06]\n",
      "\n",
      "Loss\n",
      " 11.486924483755933\n"
     ]
    }
   ],
   "source": [
    "loss = CrossEntropyLoss()\n",
    "softmax = Softmax()\n",
    "layer1 = MultiplyLayer(4, 3)\n",
    "\n",
    "print(\"X\\n\", X[0])\n",
    "print(\"\\ny\\n\", y[0])\n",
    "\n",
    "layer1.forward(X[0])\n",
    "print(\"\\nLayer 1 Output\\n\", layer1.output)\n",
    "print(\"\\nLayer 1 Weights\\n\", layer1.weights)\n",
    "\n",
    "softmax.forward(layer1.output)\n",
    "print(\"\\nPrediction\\n\", softmax.output)\n",
    "\n",
    "loss.forward(softmax.output, y[0])\n",
    "print(\"\\nLoss\\n\", loss.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a5fd29a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient of Loss w.r.t Prediction\n",
      " [-97433.41335777     -0.             -0.        ]\n",
      "\n",
      "Local Gradient of Softmax Output w.r.t Layer1 Output\n",
      " [[ 1.02633142e-05 -1.02632894e-05 -2.48370926e-11]\n",
      " [-1.02632894e-05  1.26832214e-05 -2.41993202e-06]\n",
      " [-2.48370926e-11 -2.41993202e-06  2.41995686e-06]]\n",
      "\n",
      "Gradient of Loss w.r.t Layer1 Output\n",
      " [-9.99989737e-01  9.99987317e-01  2.41996271e-06]\n",
      "\n",
      "Weights Gradient\n",
      " [[-5.09994766e+00  5.09993531e+00  1.23418098e-05]\n",
      " [-3.49996408e+00  3.49995561e+00  8.46986950e-06]\n",
      " [-1.39998563e+00  1.39998224e+00  3.38794780e-06]\n",
      " [-1.99997947e-01  1.99997463e-01  4.83992543e-07]]\n",
      "\\Bias Gradient\n",
      " [-9.99989737e-01  9.99987317e-01  2.41996271e-06]\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(\"\\nGradient of Loss w.r.t Prediction\\n\", loss.gradient)\n",
    "\n",
    "softmax.backward(loss.gradient)\n",
    "print(\"\\nLocal Gradient of Softmax Output w.r.t Layer1 Output\\n\", softmax.local_gradient)\n",
    "print(\"\\nGradient of Loss w.r.t Layer1 Output\\n\", softmax.gradient)\n",
    "\n",
    "layer1.backward(softmax.gradient)\n",
    "print(\"\\nWeights Gradient\\n\", layer1.weights_gradient)\n",
    "print(\"\\Bias Gradient\\n\", layer1.bias_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a60ce5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            MultiplyLayer(4, 6),\n",
    "            ReLU(),\n",
    "            MultiplyLayer(6, 4),\n",
    "            ReLU(),\n",
    "            MultiplyLayer(4, 3),\n",
    "            Softmax()\n",
    "        ]\n",
    "        self.loss_func = CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x, y=None, logging=False):\n",
    "        z = x\n",
    "        if logging:\n",
    "            print(\"Input: \", z, \"Label: \", y)\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            z = layer.forward(z)\n",
    "            if logging:\n",
    "                print(\"\\n\", type(layer), \"\\nOutput: \", z, \"\\nShape: \", z.shape)\n",
    "                \n",
    "        if y is not None:\n",
    "            self.loss = self.loss_func.forward(z, y)\n",
    "            if logging:\n",
    "                print(\"\\nLoss: \", self.loss)\n",
    "                \n",
    "        return z\n",
    "    \n",
    "    def backward(self, logging=False):\n",
    "        dz = self.loss_func.backward()\n",
    "        if logging:\n",
    "            print(\"dL/dpred: \", dz)\n",
    "            \n",
    "        for layer in self.layers[::-1]:\n",
    "            dz = layer.backward(dz)\n",
    "            if logging:\n",
    "                print(\"\\n\", type(layer), \"\\nGradient: \", dz)\n",
    "    \n",
    "    def SGD(self, X, y, epochs, logging=False, lr=0.01):\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                self.forward(X[i], y[i])\n",
    "                self.backward()\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, MultiplyLayer):\n",
    "                        layer.weights -= lr * layer.weights_gradient\n",
    "                        layer.bias -= lr * layer.bias_gradient\n",
    "\n",
    "            if logging:\n",
    "                n_accurate = 0\n",
    "                for i in range(len(X)):\n",
    "                    pred = np.argmax(self.forward(X[i]))\n",
    "                    label = np.argmax(y[i])\n",
    "                    if pred == label:\n",
    "                        n_accurate += 1\n",
    "                print(\"Epoch \", e, \"\\n\", \"-\"*50, \"\\n\")\n",
    "                print(\"Accuracy: \", n_accurate/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0803c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "94a160f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  1 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  2 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  3 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  4 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  5 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  6 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  7 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  8 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  9 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  10 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  11 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  12 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  13 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  14 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  15 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  16 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  17 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  18 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  19 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  20 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  21 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  22 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  23 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  24 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  25 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  26 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  27 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  28 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  29 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  30 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  31 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  32 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  33 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  34 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  35 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  36 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  37 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  38 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  39 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  40 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  41 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  42 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  43 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  44 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  45 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  46 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  47 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  48 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  49 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  50 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  51 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  52 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  53 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  54 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  55 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  56 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  57 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  58 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  59 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  60 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  61 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  62 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  63 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  64 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  65 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  66 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  67 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  68 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  69 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  70 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  71 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  72 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  73 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  74 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  75 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  76 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  77 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  78 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  79 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  80 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  81 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  82 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  83 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  84 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  85 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  86 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  87 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  88 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  89 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  90 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  91 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  92 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  93 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  94 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  95 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  96 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  97 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  98 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n",
      "Epoch  99 \n",
      " -------------------------------------------------- \n",
      "\n",
      "Accuracy:  0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "nn.SGD(X, y, 100, logging=True, lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "6605c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = process(test)\n",
    "n_accurate = 0\n",
    "for i in range(len(X_test)):\n",
    "    pred = np.argmax(nn.forward(X_test[i]))\n",
    "    label = np.argmax(y_test[i])\n",
    "    if pred == label:\n",
    "        n_accurate += 1\n",
    "print(\"Test Accuracy: \", n_accurate/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyLayer2(Layer):\n",
    "    def __init__(self, input_size, hidden_units):\n",
    "        self.weights = np.random.normal(size=(input_size, hidden_units))\n",
    "        self.bias = np.random.normal(size=hidden_units)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(self.weights.transpose(), self.input) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input_gradient):\n",
    "        self.weights_gradient = np.outer(self.input, input_gradient)\n",
    "        self.bias_gradient = input_gradient\n",
    "        self.input_gradient = np.dot(self.weights, input_gradient)\n",
    "        return self.input_gradient\n",
    "    \n",
    "class ReLU2(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        map_grad = np.vectorize(lambda x: 1 if x > 0 else 0)\n",
    "        self.local_gradient = map_grad(self.input).astype(float)\n",
    "        self.output = np.maximum(np.zeros(input.shape), input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input_gradient):\n",
    "        self.gradient = self.local_gradient * input_gradient # in the forward pass, zeros are handled, so the same zeros will still be zeros in the gradients\n",
    "        return self.gradient\n",
    "    \n",
    "class Softmax2(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        input_exp = np.exp(self.input)\n",
    "        sum_exp = sum(input_exp)\n",
    "        self.output = input_exp / sum_exp\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, input_gradient):\n",
    "        output_size = len(self.output)\n",
    "        self.local_gradient = np.ones((output_size, output_size))\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                if i == j:\n",
    "                    self.local_gradient[i][j] = self.output[i] - self.output[i]**2\n",
    "                else:\n",
    "                    self.local_gradient[i][j] = -1 * self.output[i] * self.output[j]\n",
    "        self.gradient = np.dot(self.local_gradient, input_gradient)\n",
    "        return self.gradient\n",
    "    \n",
    "class CrossEntropyLoss2(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input, label):\n",
    "        self.input = input\n",
    "        self.label = label\n",
    "        self.loss = -1 * np.dot(self.label, np.log(self.input))\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradient = -1 * np.divide(self.label, self.input)\n",
    "        return self.gradient\n",
    "\n",
    "class NeuralNetwork2():\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            MultiplyLayer2(4, 6),\n",
    "            ReLU2(),\n",
    "            MultiplyLayer2(6, 4),\n",
    "            ReLU2(),\n",
    "            MultiplyLayer2(4, 3),\n",
    "            Softmax2()\n",
    "        ]\n",
    "        self.loss_func = CrossEntropyLoss2()\n",
    "    \n",
    "    def forward(self, X, Y=None, logging=False):\n",
    "        Z = X\n",
    "        if logging:\n",
    "            print(\"Input Shape: \", X.shape(), \" Label Shape: \", Y.shape())\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z)\n",
    "            if logging:\n",
    "                print(\"\\n\", type(layer), \"\\nOutput: \", z, \"\\nShape: \", z.shape)\n",
    "                \n",
    "        if y is not None:\n",
    "            self.loss = self.loss_func.forward(z, y)\n",
    "            if logging:\n",
    "                print(\"\\nLoss: \", self.loss)\n",
    "                \n",
    "        return z\n",
    "    \n",
    "    def backward(self, logging=False):\n",
    "        dz = self.loss_func.backward()\n",
    "        if logging:\n",
    "            print(\"dL/dpred: \", dz)\n",
    "            \n",
    "        for layer in self.layers[::-1]:\n",
    "            dz = layer.backward(dz)\n",
    "            if logging:\n",
    "                print(\"\\n\", type(layer), \"\\nGradient: \", dz)\n",
    "    \n",
    "    def SGD(self, X, y, epochs, logging=False, lr=0.01):\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                self.forward(X[i], y[i])\n",
    "                self.backward()\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, MultiplyLayer):\n",
    "                        layer.weights -= lr * layer.weights_gradient\n",
    "                        layer.bias -= lr * layer.bias_gradient\n",
    "\n",
    "            if logging:\n",
    "                n_accurate = 0\n",
    "                for i in range(len(X)):\n",
    "                    pred = np.argmax(self.forward(X[i]))\n",
    "                    label = np.argmax(y[i])\n",
    "                    if pred == label:\n",
    "                        n_accurate += 1\n",
    "                print(\"Epoch \", e, \"\\n\", \"-\"*50, \"\\n\")\n",
    "                print(\"Accuracy: \", n_accurate/len(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
